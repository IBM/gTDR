{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "from gTDR.models import FastGCN\n",
    "from gTDR.trainers.FastGCN_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `dataset_root`: (str) The path where to save the downloaded dataset.\n",
    "* `fast`: (bool) Whether to use the FastGCN method.\n",
    "* `save_results`: (bool) Whether to save the training and testing results.\n",
    "* `save_path`: (str) The path where to save the trained model and results.\n",
    "* `use_cuda`: (bool) Whether to use CUDA for GPU acceleration.\n",
    "* `seed`: (int) The random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/FastGCN_Cora_parameters.yaml\"\n",
    "with open(config_filename) as f:\n",
    "    configs = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "args = types.SimpleNamespace(**configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_cuda = (torch.cuda.is_available() and args.use_cuda)\n",
    "if args.use_cuda:\n",
    "    args.device = 'cuda'\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "print (\"use CUDA:\", args.use_cuda, \"- device:\", args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if args.use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment (train loss, validation loss, test accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"FastGCN_ParameterTuning\", name=\"Cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search (see [wandb's documentation](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do parameter search, you need to specify your method, metric to base on, as well as range of (which) parameters to search for. In this example, we use the random method to maximize test accuracy by searching for learning rate in the range from 0.0001 to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',  # bayes, grid\n",
    "    'metric': {\n",
    "      'name': 'Test Accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"FastGCN_ParameterTuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we use `Cora` from the PyG `Planetoid` collection. For customized datasets, please refer to PyG dataset [documentation](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root=args.dataset_root, name=args.dataset, \n",
    "                    transform=T.ToSparseTensor(remove_edge_index=False), \n",
    "                    split='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may specify these model parameters in config:\n",
    "\n",
    "* `hidden_dims`: (int) The dimensionality of the hidden layers in the graph convolutional network (GCN).\n",
    "\n",
    "* `num_layers`: (int) The number of layers in the GCN.\n",
    "\n",
    "* `dropout`: (float) The dropout rate for regularization.\n",
    "\n",
    "* `batch_norm`: (bool) Whether to use batch normalization in the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastGCN(args, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `normalize_features`: (bool) If set to True, the input features to the model will be normalized (mean-centered and divided by their standard deviation). If set to False, the features will be used as is.\n",
    "\n",
    "* `init_batch`: (int) Specifies the number of samples to be included in the initial batch of training data. This is the batch size at the input layer of the FastGCN model during training.\n",
    "\n",
    "* `sample_size`: (int) Specifies the number of samples to be used in each training step after the initial batch. This controls the sample size for the hidden layers in the FastGCN model during training.\n",
    "\n",
    "* `scale_factor`: (float) A factor by which the batch size is increased at each layer of the Graph Convolutional Network (GCN). This can be used to progressively increase the batch size at deeper layers.\n",
    "\n",
    "* `epochs`: (int) The total number of training epochs. An epoch is a complete pass through the entire training dataset.\n",
    "\n",
    "* `lr`: (float) The learning rate for the optimizer. This controls how much the model parameters are updated in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "* `early_stop`: (int) The number of epochs with no improvement in validation loss after which training will be stopped. This is a form of early stopping, which can prevent overfitting.\n",
    "\n",
    "* `weight_decay`: (float) The weight decay (L2 penalty) for the optimizer. This adds a regularization term to the loss function, which can help prevent overfitting.\n",
    "\n",
    "* `samp_inference`: (bool) If set to `True`, importance sampling will be used during the inference phase (i.e., when making predictions on unseen data). If set to `False`, all instances will be used.\n",
    "\n",
    "* `use_val`: (bool) If set to `True`, a validation set will be used during training. \n",
    "\n",
    "* `num_samp_inference`: (int) The number of samples to use for each inference step. This is relevant only if `samp_inference` is set to True.\n",
    "\n",
    "* `inference_init_batch`: (int) The number of samples to be included in the initial batch of inference data. This is the batch size at the input layer of the FastGCN model during inference.\n",
    "\n",
    "* `inference_sample_size`: (int) Specifies the number of samples to be used in each inference step after the initial batch. This controls the batch size for the hidden layers in the FastGCN model during inference.\n",
    "\n",
    "* `report`: (int) The number of training epochs after which a report (validation loss and test accuracy) will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "To tune hyperparameters, you need to write your own `parameter_search(self)` function to customize which hyperparameters to tune. Below is an example of searching for `lr` and consequently updating all trainer attributes that use `lr` for it to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(self):\n",
    "    run = wandb.init() # must have\n",
    "    self.run = run\n",
    "    config = run.config # must have\n",
    "\n",
    "    # parameter search customization\n",
    "    self.args.lr = config.lr\n",
    "    self.optimizer = torch.optim.Adam(params=self.model.parameters(), \n",
    "                                      lr=self.args.lr, \n",
    "                                      weight_decay=self.args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the wandb agent to start hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, lambda: trainer.train(parameter_search=parameter_search), count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
