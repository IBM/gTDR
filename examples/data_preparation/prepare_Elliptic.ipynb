{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f689cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dca019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_path = '../../data/Elliptic/elliptic_bitcoin_dataset/'\n",
    "data_out_path = '../../data/Elliptic/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd334d51",
   "metadata": {},
   "source": [
    "## Step 1: Create a file named `elliptic_txs_orig2contiguos.csv` and modify `elliptic_txs_features.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31d10f",
   "metadata": {},
   "source": [
    "First, read the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original dataset\n",
    "txs_features = pd.read_csv(os.path.join(data_in_path, 'elliptic_txs_features.csv'), header=None)\n",
    "txs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719c1b6",
   "metadata": {},
   "source": [
    "Next, create a new dataframe that stores the original ID (in the first column) and the contiguous ID (which is simply the line number):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca800417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dataframe\n",
    "id_mapping_df = pd.DataFrame({\n",
    "    'originalId': txs_features.iloc[:, 0],\n",
    "    'contiguousId': range(len(txs_features))\n",
    "})\n",
    "\n",
    "# Save the mapping dataframe to a csv file\n",
    "id_mapping_df.to_csv(os.path.join(data_out_path, 'elliptic_txs_orig2contiguos.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a00d05",
   "metadata": {},
   "source": [
    "Finally, modify the original dataframe by replacing the first column with the line number and converting the first two columns to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the first column with the line number and convert to float\n",
    "txs_features.iloc[:, 0] = id_mapping_df['contiguousId'].astype(float)\n",
    "\n",
    "# Convert the second column to float\n",
    "txs_features.iloc[:, 1] = txs_features.iloc[:, 1].astype(float)\n",
    "\n",
    "# Save the modified dataframe to a new csv file\n",
    "txs_features.to_csv(os.path.join(data_out_path, 'modified_elliptic_txs_features.csv'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dcfc0",
   "metadata": {},
   "source": [
    "This should create two new CSV files:\n",
    "\n",
    "* `elliptic_txs_orig2contiguos.csv`: This file contains the mapping from the original ID to the contiguous ID.\n",
    "\n",
    "* `modified_elliptic_txs_features.csv`: This file is a modified version of your original dataset, where the first number in each line is replaced by the line number (converted to a float), and the second number is also converted to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b58157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "txs_orig2contiguos = pd.read_csv(os.path.join(data_out_path, 'elliptic_txs_orig2contiguos.csv'))\n",
    "txs_orig2contiguos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_txs_features = pd.read_csv(os.path.join(data_out_path, 'modified_elliptic_txs_features.csv'), header=None)\n",
    "modified_txs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e785a34",
   "metadata": {},
   "source": [
    "## Step 2: Modify `elliptic_txs_classes.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f272d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.read_csv(os.path.join(data_in_path, 'elliptic_txs_classes.csv'))\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d4a1f",
   "metadata": {},
   "source": [
    "Replace the `txId` values with the corresponding contiguous ids from our `id_mapping_df` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the classes dataframe with the mapping dataframe\n",
    "df_classes = df_classes.merge(id_mapping_df, left_on='txId', right_on='originalId', how='left')\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the originalId and txId columns\n",
    "df_classes.drop(columns=['txId', 'originalId'], inplace=True)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename contiguousId to txId\n",
    "df_classes.rename(columns={'contiguousId': 'txId'}, inplace=True)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72326a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = df_classes[['txId', 'class']]\n",
    "df_classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38de04",
   "metadata": {},
   "source": [
    "Then, replace the class values according to the rules you provided (-1.0 for 'unknown', 1.0 for '1', and 0 for '2'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbec2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping for classes\n",
    "class_mapping = {'unknown': -1.0, '1': 1.0, '2': 0}\n",
    "\n",
    "# Replace the class values\n",
    "df_classes['class'] = df_classes['class'].map(class_mapping)\n",
    "df_classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b629d34",
   "metadata": {},
   "source": [
    "Finally, save the modified classes data to a new CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca52a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified classes dataframe to a new csv file\n",
    "df_classes.to_csv(os.path.join(data_out_path, 'modified_elliptic_txs_classes.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe8b0",
   "metadata": {},
   "source": [
    "This should create a new CSV file `modified_elliptic_txs_classes.csv`. In this file, the `txId` values are replaced by the contiguous ids, and the class values are converted to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d70501",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_txs_classes = pd.read_csv(os.path.join(data_out_path, 'modified_elliptic_txs_classes.csv'))\n",
    "modified_txs_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea9d4a",
   "metadata": {},
   "source": [
    "## Step 3: Create a file named `elliptic_txs_nodetime.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e604b8",
   "metadata": {},
   "source": [
    "Continuing from the previous steps, we will now use the previously modified `txs_features` dataframe that holds the features to extract the txId and timestep data.\n",
    "\n",
    "Let's create the `elliptic_txs_nodetime.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_txs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a29bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that contains the new node id and the timestamp\n",
    "df_nodetime = pd.DataFrame({\n",
    "    'txId': modified_txs_features.iloc[:, 0].astype(int),  # the new node id\n",
    "    'timestep': (modified_txs_features.iloc[:, 1] - 1).astype(int)  # the timestamp, shifted down by 1\n",
    "})\n",
    "\n",
    "# Save the nodetime dataframe to a csv file\n",
    "df_nodetime.to_csv(os.path.join(data_out_path, 'elliptic_txs_nodetime.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d343021",
   "metadata": {},
   "source": [
    "This will create a new CSV file `elliptic_txs_nodetime.csv`. The txId values in this file are the contiguous ids, and the timestep values are the timestamps from the original `elliptic_txs_features.csv`, shifted down by 1 as per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b28b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_txs_nodetime = pd.read_csv(os.path.join(data_out_path, 'elliptic_txs_nodetime.csv'))\n",
    "elliptic_txs_nodetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_txs_nodetime['timestep'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034b97b",
   "metadata": {},
   "source": [
    "## Step 4: Modify elliptic_txs_edgelist.csv and rename it to elliptic_txs_edgelist_timed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798bfb1",
   "metadata": {},
   "source": [
    "First, we read the edgelist data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the edgelist data\n",
    "df_edgelist = pd.read_csv(os.path.join(data_in_path, 'elliptic_txs_edgelist.csv'))\n",
    "df_edgelist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bf5fd",
   "metadata": {},
   "source": [
    "Next, we replace the `txId1` and `txId2` values with the corresponding new ids from our `id_mapping_df` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the edge list dataframe with the mapping dataframe\n",
    "df_edgelist = df_edgelist.merge(id_mapping_df, left_on='txId1', right_on='originalId', how='left')\n",
    "\n",
    "# Drop the originalId and txId1 columns\n",
    "df_edgelist.drop(columns=['txId1', 'originalId'], inplace=True)\n",
    "\n",
    "# Rename contiguousId to txId1\n",
    "df_edgelist.rename(columns={'contiguousId': 'txId1'}, inplace=True)\n",
    "\n",
    "# Repeat the same for txId2\n",
    "df_edgelist = df_edgelist.merge(id_mapping_df, left_on='txId2', right_on='originalId', how='left')\n",
    "df_edgelist.drop(columns=['txId2', 'originalId'], inplace=True)\n",
    "df_edgelist.rename(columns={'contiguousId': 'txId2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43695cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e6c41",
   "metadata": {},
   "source": [
    "Then, we need to add a timestep column to the dataframe. We can extract this from the `df_nodetime` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c064586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the edge list dataframe with the nodetime dataframe\n",
    "df_edgelist = df_edgelist.merge(df_nodetime, left_on='txId1', right_on='txId', how='left')\n",
    "\n",
    "# Drop the unnecessary txId column\n",
    "df_edgelist.drop(columns=['txId'], inplace=True)\n",
    "\n",
    "# Rename timestep to timestep (float)\n",
    "df_edgelist['timestep'] = df_edgelist['timestep'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be23801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified edgelist dataframe to a new csv file\n",
    "df_edgelist.to_csv(os.path.join(data_out_path, 'elliptic_txs_edgelist_timed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad4aa8",
   "metadata": {},
   "source": [
    "This should create a new CSV file `elliptic_txs_edgelist_timed.csv`. In this file, the `txId1` and `txId2` values are replaced by the new node ids, and a timestep column is added which indicates the timestamp for the corresponding edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcadbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_txs_edgelist_timed = pd.read_csv(os.path.join(data_out_path, 'elliptic_txs_edgelist_timed.csv'))\n",
    "elliptic_txs_edgelist_timed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed85fd8",
   "metadata": {},
   "source": [
    "## Final: move all files into `elliptic_compress` folder and turn it into a tar.gz file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afd976",
   "metadata": {},
   "source": [
    "Remove NaN entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5db2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'elliptic_txs_edgelist_timed.csv',\n",
    "    'elliptic_txs_nodetime.csv',\n",
    "    'modified_elliptic_txs_classes.csv',\n",
    "    'modified_elliptic_txs_features.csv',\n",
    "]\n",
    "\n",
    "# iterate over each file\n",
    "for file_name in files:\n",
    "    # read the csv file into a pandas DataFrame\n",
    "    if file_name == 'modified_elliptic_txs_features.csv':\n",
    "        df = pd.read_csv(os.path.join(data_out_path, file_name), header=None)\n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(data_out_path, file_name))\n",
    "\n",
    "    # check if there are any NaN values in the DataFrame\n",
    "    if df.isna().any().any():\n",
    "        print(f\"The file {file_name} contains NaN values.\")\n",
    "    else:\n",
    "        print(f\"The file {file_name} does not contain any NaN values.\")\n",
    "        \n",
    "#     # drop any rows that contain NaN\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # write the DataFrame back to the csv file\n",
    "#     if file_name == 'modified_elliptic_txs_features.csv':\n",
    "#         df.to_csv(file_name, index=False, header=None)\n",
    "#     else:\n",
    "#         df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800be3b",
   "metadata": {},
   "source": [
    "Compress into tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the output tar.gz file name\n",
    "output_filename = \"elliptic_bitcoin_dataset_cont_updated.tar.gz\"\n",
    "\n",
    "files = [\n",
    "    'elliptic_txs_edgelist_timed.csv',\n",
    "    'elliptic_txs_nodetime.csv',\n",
    "    'modified_elliptic_txs_classes.csv',\n",
    "    'modified_elliptic_txs_features.csv',\n",
    "]\n",
    "\n",
    "with tarfile.open(os.path.join(data_out_path, output_filename), \"w:gz\") as tar:\n",
    "    # iterate over each item in the directory\n",
    "    for item in files:\n",
    "        print('item:', item)\n",
    "        # add the item (file or directory) to the tar.gz file\n",
    "        tar.add(os.path.join(data_out_path, item), arcname=os.path.basename(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
