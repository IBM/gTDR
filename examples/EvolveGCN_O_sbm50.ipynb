{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78765bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import types\n",
    "import argparse\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import tarfile\n",
    "import itertools\n",
    "import yaml\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41af5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "import gTDR.utils.EvolveGCN as utils\n",
    "from gTDR.trainers.EvolveGCN_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec17db",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919feed",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `folder`: (str) The path of the dataset.\n",
    "* `use_cuda`: (bool) Whether to use CUDA for GPU acceleration.\n",
    "* `use_logfile`: (bool) If true, we save the output in a log file, if false the result is in stdout.\n",
    "* `save_results`: (bool) Whether to save the training and testing results.\n",
    "* `save_path`: (str) The path where to save the trained model and results.\n",
    "* `seed`: (int) The random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/EvolveGCN_O_sbm50_parameters.yaml\"\n",
    "with open(config_filename) as f:\n",
    "    configs = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "args = types.SimpleNamespace(**configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef5714",
   "metadata": {},
   "source": [
    "Use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85382a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_cuda = (torch.cuda.is_available() and args.use_cuda)\n",
    "args.device='cpu'\n",
    "if args.use_cuda:\n",
    "    args.device='cuda'\n",
    "print (\"use CUDA:\", args.use_cuda, \"- device:\", args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d435c38",
   "metadata": {},
   "source": [
    "Set seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ea79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed3f24",
   "metadata": {},
   "source": [
    "Complete the specification of `args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa086e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = utils.build_random_hyper_params(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d278f35",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment (train loss, validation loss, and `target_measure` specified in config). See the config file for choices of `target_measure`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"EvolveGCN_O\", name=\"sbm50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d238ade",
   "metadata": {},
   "source": [
    "## Data (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a28fd3",
   "metadata": {},
   "source": [
    "In this demo, we use the `sbm50` dataset.\n",
    "\n",
    "**First, unzip dataset located at [../data/sbm50/](../data/sbm50/):**\n",
    "\n",
    "`tar -xvzf ../data/sbm50/sbm_50t_1000n_adj.csv.tar.gz -C ../data/sbm50/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da815ad",
   "metadata": {},
   "source": [
    "## Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409141bd",
   "metadata": {},
   "source": [
    "Next, define a dataset class that contains class memebers `node_feats` and `edges` for temporal link prediction.\n",
    "\n",
    "Inside this class, the unzipped file `sbm_50t_1000n_adj.csv` is read to populate the class members.\n",
    "\n",
    "* `node_feats` is a tensor of node features.\n",
    "* `edges` is a dictionary with `idx` and `vals` as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sbm_dataset():\n",
    "    def __init__(self, args):\n",
    "        assert args.task in ['link_pred'], 'sbm only implements link_pred'\n",
    "        self.ecols = utils.Namespace({'FromNodeId': 0,\n",
    "                                      'ToNodeId': 1,\n",
    "                                      'Weight': 2,\n",
    "                                      'TimeStep': 3\n",
    "                                     })\n",
    "        args.sbm_args = utils.Namespace(args.sbm_args)\n",
    "\n",
    "        #build edge data structure\n",
    "        edges = self.load_edges(args.sbm_args)\n",
    "        timesteps = utils.aggregate_by_time(edges[:,self.ecols.TimeStep], args.sbm_args.aggr_time)\n",
    "        self.max_time = timesteps.max()\n",
    "        self.min_time = timesteps.min()\n",
    "        print ('TIME', self.max_time, self.min_time )\n",
    "        edges[:,self.ecols.TimeStep] = timesteps\n",
    "\n",
    "        edges[:,self.ecols.Weight] = self.cluster_negs_and_positives(edges[:,self.ecols.Weight])\n",
    "        self.num_classes = edges[:,self.ecols.Weight].unique().size(0)\n",
    "\n",
    "        self.edges = self.edges_to_sp_dict(edges)\n",
    "        \n",
    "        #random node features\n",
    "        self.num_nodes = int(self.get_num_nodes(edges))\n",
    "        self.feats_per_node = args.sbm_args.feats_per_node\n",
    "        self.nodes_feats = torch.rand((self.num_nodes,self.feats_per_node))\n",
    "\n",
    "        self.num_non_existing = self.num_nodes ** 2 - edges.size(0)\n",
    "\n",
    "    def cluster_negs_and_positives(self,ratings):\n",
    "        pos_indices = ratings >= 0\n",
    "        neg_indices = ratings < 0\n",
    "        ratings[pos_indices] = 1\n",
    "        ratings[neg_indices] = 0\n",
    "        return ratings\n",
    "\n",
    "    def prepare_node_feats(self,node_feats):\n",
    "        node_feats = node_feats[0]\n",
    "        return node_feats\n",
    "\n",
    "    def edges_to_sp_dict(self,edges):\n",
    "        idx = edges[:,[self.ecols.FromNodeId,\n",
    "                       self.ecols.ToNodeId,\n",
    "                       self.ecols.TimeStep]]\n",
    "\n",
    "        vals = edges[:,self.ecols.Weight]\n",
    "        return {'idx': idx,\n",
    "                'vals': vals}\n",
    "\n",
    "    def get_num_nodes(self,edges):\n",
    "        all_ids = edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        num_nodes = all_ids.max() + 1\n",
    "        return num_nodes\n",
    "\n",
    "    def load_edges(self,sbm_args, starting_line = 1):\n",
    "        file = os.path.join(sbm_args.folder,sbm_args.edges_file)\n",
    "        with open(file) as f:\n",
    "            lines = f.read().splitlines()\n",
    "        edges = [[float(r) for r in row.split(',')] for row in lines[starting_line:]]\n",
    "        edges = torch.tensor(edges,dtype = torch.long)\n",
    "        return edges\n",
    "\n",
    "    def make_contigous_node_ids(self,edges):\n",
    "        new_edges = edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        _, new_edges = new_edges.unique(return_inverse=True)\n",
    "        edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]] = new_edges\n",
    "        return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8443f1",
   "metadata": {},
   "source": [
    "Create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9347737",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sbm_args = args.sbm50_args\n",
    "dataset = sbm_dataset(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760bb02",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1bdb6",
   "metadata": {},
   "source": [
    "Build model. In this demo, `args.model=egcn_o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.build_model(args, dataset, task='link_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b6749",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd924443",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `train_proportion`: (float) The proportion of the dataset used for training. \n",
    "\n",
    "* `dev_proportion`: (float) The proportion of the dataset used for validation.\n",
    "\n",
    "* `num_epochs`: (int) The number of epochs to train the model.\n",
    "\n",
    "* `steps_accum_gradients`: (int) The number of steps to accumulate gradients before updating the model parameters. \n",
    "\n",
    "* `learning_rate`: (float) The learning rate for the Adam optimizer.\n",
    "\n",
    "* `early_stop_patience`: (int) The number of epochs with no improvement after which training will be stopped. \n",
    "\n",
    "* `adj_mat_time_window`: (int) The time window to create the adjacency matrix for each time step. This parameter is not used directly in the trainer but it might be used in some other parts of the code.\n",
    "\n",
    "* `data_loading_params` \n",
    "    * `batch_size`: (int) number of data samples propagated through the network at once. \n",
    "    * `num_workers`: (int) number of subprocesses to use for data loading. The main benefit of using multiple processes is that they can use separate memory and CPUs to load data in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e8c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(args, model=model)\n",
    "trainer.train(use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a00f1",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc5af0",
   "metadata": {},
   "source": [
    "Load the best check point and perform testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
