{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import types\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tarfile\n",
    "import yaml\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "import gTDR.utils.EvolveGCN as utils\n",
    "from gTDR.trainers.EvolveGCN_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d3a15",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0943839",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `folder`: (str) The path of the dataset.\n",
    "* `use_cuda`: (bool) Whether to use CUDA for GPU acceleration.\n",
    "* `use_logfile`: (bool) If true, we save the output in a log file, if false the result is in stdout.\n",
    "* `save_results`: (bool) Whether to save the training and testing results.\n",
    "* `save_path`: (str) The path where to save the trained model and results.\n",
    "* `seed`: (int) The random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/EvolveGCN_H_Elliptic_parameters.yaml\"\n",
    "with open(config_filename) as f:\n",
    "    configs = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "args = types.SimpleNamespace(**configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c70ba9",
   "metadata": {},
   "source": [
    "Use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_cuda = (torch.cuda.is_available() and args.use_cuda)\n",
    "args.device='cpu'\n",
    "if args.use_cuda:\n",
    "    args.device='cuda'\n",
    "print (\"use CUDA:\", args.use_cuda, \"- device:\", args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615b327",
   "metadata": {},
   "source": [
    "Set seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b84c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12745f",
   "metadata": {},
   "source": [
    "Complete the specification of `args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = utils.build_random_hyper_params(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07baf698",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment (train loss, validation loss, and `target_measure` specified in config). See the config file for choices of `target_measure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7832a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"EvolveGCN_H\", name=\"Elliptic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cadd4",
   "metadata": {},
   "source": [
    "## Data (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9893c66",
   "metadata": {},
   "source": [
    "In this demo, we use the `Elliptic` dataset.\n",
    "\n",
    "**First, download the dataset from [Kaggle](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set). The download results in a folder `elliptic_bitcoin_dataset/`. Place it under [../data/Elliptic/](../data/Elliptic/).**\n",
    "\n",
    "**Then, run the notebook [./data_preparation/prepare_Elliptic.ipynb](./data_preparation/prepare_Elliptic.ipynb) to create tar file `elliptic_bitcoin_dataset_cont_updated.tar.gz` under [../data/Elliptic/](../data/Elliptic/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7cc34",
   "metadata": {},
   "source": [
    "## Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de13f8c",
   "metadata": {},
   "source": [
    "Next, define a dataset class that contains class memebers `nodes`, `node_feats`, `nodes_labels_times`, `edges` for temporal node classification.\n",
    "\n",
    "Inside this class, the tar file `elliptic_bitcoin_dataset_cont_updated.tar.gz` is read to populate the class members.\n",
    "\n",
    "* `nodes` is a list of nodes.\n",
    "* `node_feats` is a tensor of node features.\n",
    "* `nodes_labels_times` is a tensor of node labels, each represented as `[node id, label, time]`.\n",
    "* `edges` is a dictionary with `idx` and `vals` as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elliptic_Temporal_Dataset():\n",
    "    def __init__(self,args):\n",
    "        args.elliptic_args = utils.Namespace(args.elliptic_args)\n",
    "        \n",
    "        tar_file = os.path.join(args.elliptic_args.folder, args.elliptic_args.tar_file)\n",
    "        tar_archive = tarfile.open(tar_file, 'r:gz')\n",
    "\n",
    "        self.nodes_labels_times = self.load_node_labels(args.elliptic_args, tar_archive)\n",
    "\n",
    "        self.edges = self.load_transactions(args.elliptic_args, tar_archive)\n",
    "\n",
    "        self.nodes, self.nodes_feats = self.load_node_feats(args.elliptic_args, tar_archive)\n",
    "\n",
    "    def load_node_feats(self, elliptic_args, tar_archive):\n",
    "        data = utils.load_data_from_tar(elliptic_args.feats_file, tar_archive, starting_line=0)\n",
    "        nodes = data\n",
    "\n",
    "        nodes_feats = nodes[:,1:]\n",
    "\n",
    "        self.num_nodes = len(nodes)\n",
    "        self.feats_per_node = data.size(1) - 1\n",
    "\n",
    "        return nodes, nodes_feats.float()\n",
    "\n",
    "    def load_node_labels(self, elliptic_args, tar_archive):\n",
    "        labels = utils.load_data_from_tar(elliptic_args.classes_file, tar_archive, replace_unknow=True).long()\n",
    "        times = utils.load_data_from_tar(elliptic_args.times_file, tar_archive, replace_unknow=True).long()\n",
    "        lcols = utils.Namespace({'nid': 0, 'label': 1})\n",
    "        tcols = utils.Namespace({'nid':0, 'time':1})\n",
    "        \n",
    "        nodes_labels_times =[]\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i,[lcols.label]].long()\n",
    "            if label>=0:\n",
    "                nid=labels[i,[lcols.nid]].long()\n",
    "                time=times[nid,[tcols.time]].long()\n",
    "                nodes_labels_times.append([nid , label, time])\n",
    "        nodes_labels_times = torch.tensor(nodes_labels_times)\n",
    "\n",
    "        return nodes_labels_times\n",
    "\n",
    "    def load_transactions(self, elliptic_args, tar_archive):\n",
    "        data = utils.load_data_from_tar(elliptic_args.edges_file, tar_archive, type_fn=float, tensor_const=torch.LongTensor)\n",
    "        tcols = utils.Namespace({'source': 0, 'target': 1, 'time': 2})\n",
    "\n",
    "        data = torch.cat([data,data[:,[1,0,2]]])\n",
    "\n",
    "        self.max_time = data[:,tcols.time].max()\n",
    "        self.min_time = data[:,tcols.time].min()\n",
    "\n",
    "        return {'idx': data, 'vals': torch.ones(data.size(0))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3388337",
   "metadata": {},
   "source": [
    "Create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Elliptic_Temporal_Dataset(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd9a99",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50791f47",
   "metadata": {},
   "source": [
    "Build model. In this demo, `args.model=egcn_h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.build_model(args, dataset, task='node_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205a6eb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e9fa5",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `train_proportion`: (float) The proportion of the dataset used for training. \n",
    "\n",
    "* `dev_proportion`: (float) The proportion of the dataset used for validation.\n",
    "\n",
    "* `num_epochs`: (int) The number of epochs to train the model.\n",
    "\n",
    "* `steps_accum_gradients`: (int) The number of steps to accumulate gradients before updating the model parameters. \n",
    "\n",
    "* `learning_rate`: (float) The learning rate for the Adam optimizer.\n",
    "\n",
    "* `early_stop_patience`: (int) The number of epochs with no improvement after which training will be stopped. \n",
    "\n",
    "* `adj_mat_time_window`: (int) The time window to create the adjacency matrix for each time step. This parameter is not used directly in the trainer but it might be used in some other parts of the code.\n",
    "\n",
    "* `data_loading_params` \n",
    "    * `batch_size`: (int) number of data samples propagated through the network at once. \n",
    "    * `num_workers`: (int) number of subprocesses to use for data loading. The main benefit of using multiple processes is that they can use separate memory and CPUs to load data in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e7747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(args, model=model)\n",
    "trainer.train(use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf25096",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03351df",
   "metadata": {},
   "source": [
    "Load the best check point and perform testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5511f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e662126",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
