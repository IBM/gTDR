{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de333af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "import types\n",
    "import collections\n",
    "from itertools import repeat\n",
    "from typing import List, Dict, Any\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "from gTDR.datasets import FastDataset\n",
    "import gTDR.utils.SALIENT as utils \n",
    "from gTDR.utils.SALIENT.fast_trainer.utils import Timer\n",
    "from gTDR.trainers.SALIENT_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b8b35",
   "metadata": {},
   "source": [
    "Specify model. Opther options: `GAT`, `GIN`, `SAGEResInception`, `SAGEClassic`, `JKNet`, `GCN`, `ARMA`. Should be consistent with that specified in `../configs/SALIENT_ogbn_arxiv_single_machine_parameters.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gTDR.models.SALIENT import SAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc3904",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312936f",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `dataset_root`: (str) The path where to save the downloaded dataset.\n",
    "* `output_root`: (str) The path where to save the trained model and results.\n",
    "* `save_results`: (bool) Whether to save the training and testing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/SALIENT_ogbn_arxiv_single_machine_parameters.yaml\"\n",
    "with open(config_filename) as f:\n",
    "    configs = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "args = types.SimpleNamespace(**configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932354f9",
   "metadata": {},
   "source": [
    "Complete the specification of `args` and set up multi-GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = utils.setup(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91124f4c",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3953f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"SALIENT\", name=\"ogbn-arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e319f",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb92417",
   "metadata": {},
   "source": [
    "In this demo, we use an `ogb` dataset.\n",
    "\n",
    "SALIENT defines a dataset class `FastDataset`. It supports loading `ogb` datasets. To use custom data, one needs to modify `FastDataset` (see [../gTDR/datasets/SALIENT_Dataset.py](../gTDR/datasets/SALIENT_Dataset.py)).\n",
    "\n",
    "`FastDataset` includes these properties:\n",
    "\n",
    "* `name`: (str) The name of the dataset.\n",
    "\n",
    "* `x`: (torch.Tensor) A tensor containing the feature vectors of the nodes in the graph.\n",
    "\n",
    "* `y`: (torch.Tensor) A tensor containing the labels of the nodes in the graph.\n",
    "\n",
    "* `rowptr`: (torch.Tensor) A tensor containing the row pointers of the adjacency matrix of the graph in Compressed Sparse Row (CSR) format.\n",
    "\n",
    "* `col`: (torch.Tensor) A tensor containing the column indices of the adjacency matrix of the graph in CSR format.\n",
    "\n",
    "* `split_idx`: (Mapping[str, torch.Tensor]) A dictionary containing the indices for splitting the dataset into training, validation, and testing sets. The keys are strings (\"train\", \"valid\", \"test\") and the values are tensors of indices.\n",
    "\n",
    "* `meta_info`: (Mapping[str, Any]) A dictionary containing additional metadata about the dataset. The keys are strings and the values can be of any type.\n",
    "\n",
    "`FastDataset` includes these methods:\n",
    "\n",
    "* `adj_t(self)`: This method constructs and returns a SparseTensor representing the adjacency matrix of the graph. The adjacency matrix is constructed using the rowptr and col attributes, which contain the row pointers and column indices of the adjacency matrix in Compressed Sparse Row (CSR) format. The `num_nodes` argument is the number of nodes in the graph. The `is_sorted=True` and `trust_data=True` arguments indicate that the data is already sorted and the data can be trusted to be in the correct format without further checks.\n",
    "\n",
    "* `share_memory_(self)`: This method moves the data of the tensor to shared memory using PyTorch's `share_memory_()` function. This is useful when you want to share data across multiple processes, like when using data parallelism in PyTorch. This is done for each attribute of the dataset object that is a tensor `(self.x, self.y, self.rowptr, self.col)`, as well as for each tensor in the `split_idx` dictionary.\n",
    "\n",
    "* `save(self, _path, name)` (optional): This method saves the fields of the dataset object to disk. It does this by looping over each field (like x, y, rowptr, etc.) and saving each field as a separate file in the processed data directory. The `_path` argument is the base directory where the data should be saved, and `name` is the name of the dataset. The processed data will be saved in a subdirectory named after the dataset under the `_path` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('Loading dataset'):\n",
    "    dataset = FastDataset.from_path(args.dataset_root, args.dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627efc1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532fd46",
   "metadata": {},
   "source": [
    "You may specify these model parameters in config:\n",
    "\n",
    "* `hidden_features`(int): This parameter specifies the number of hidden units in each layer of the model. This parameter defines the number of output channels, i.e., the dimension of the output feature vectors produced by each layer. The higher the number of hidden features, the more complex patterns the model can capture, but it may also increase the risk of overfitting and require more computational resources.\n",
    "\n",
    "* `num_layers` (int): This parameter specifies the number of layers in the model. The deeper the model (i.e., the more layers it has), the more complex the patterns it can theoretically learn from the data. However, as with hidden_features, increasing this parameter may also increase the risk of overfitting and the computational resources required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce6968",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563e753",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `lr` (float): The learning rate for the model's optimizer.\n",
    "\n",
    "* `epochs` (int): The number of complete passes through the entire training dataset.\n",
    "\n",
    "* `train_batch_size` (int): The number of training examples utilized in one iteration.\n",
    "\n",
    "* `test_batch_size` (int): The number of test examples utilized in one iteration.\n",
    "\n",
    "* `test_epoch_frequency` (int): The frequency, in epochs, at which the test evaluation should occur.\n",
    "\n",
    "* `test_max_num_batches` (int): The maximum number of batches to use during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdc01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, args)\n",
    "trainer.train(dataset, use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f512588",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd2728",
   "metadata": {},
   "source": [
    "Load the best check point and perform testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff6912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "trainer.test(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
