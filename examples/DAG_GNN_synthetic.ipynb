{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87284b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import types\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "from gTDR.models import DAG_GNN\n",
    "import gTDR.utils.DAG_GNN as utils \n",
    "from gTDR.trainers.DAG_GNN_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1728c9",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03cbff",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `seed`: (int) Random seed for reproducibility.\n",
    "* `use_cuda`: (bool) Whether to use CUDA for training. If True and a GPU is available, the model will be trained on the GPU.\n",
    "* `save_results`: (bool) Whether to save the training checkpoints.\n",
    "* `save_folder`: (str) Directory to save the trainer's checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/DAG_GNN_synthetic_parameters.yaml\"\n",
    "with open(config_filename) as f:\n",
    "    configs = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "args = types.SimpleNamespace(**configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef570b",
   "metadata": {},
   "source": [
    "Set seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if args.use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5edb16",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment (nll_train, kl_train, mse_train, shd_train, ELBO_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"DAG-GNN\", name=\"synthetic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f602db",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e32b33",
   "metadata": {},
   "source": [
    "In this demo, we use synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, ground_truth_G = utils.load_synthetic_data(seed=args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a62a76",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd81043",
   "metadata": {},
   "source": [
    "You may specify these model parameters in config:\n",
    "\n",
    "* `encoder`: (str) This determines the type of encoder used in the model. It can be either `mlp` for a multi-layer perceptron or `sem` for a structural equation model. This affects how the data is processed and transformed in the initial phase of the model.\n",
    "\n",
    "* `decoder`: (str) This determines the type of decoder used in the model. Similar to the encoder, it can be either `mlp` or `sem`. This affects how the latent variables are transformed back to the original data space.\n",
    "\n",
    "* `data_variable_size`: (int) This is the number of nodes in your data.\n",
    "\n",
    "* `x_dims`: (int) This is the dimensionality of the input data.\n",
    "\n",
    "* `z_dims`: (int) This is the dimensionality of the latent space\n",
    "\n",
    "* `encoder_hidden`: (int) This is the size of the hidden layer in the encoder.\n",
    "\n",
    "* `decoder_hidden`: (int) This is the size of the hidden layer in the decoder.\n",
    "\n",
    "* `encoder_dropout`: (float) This is the dropout rate applied in the encoder.\n",
    "\n",
    "* `decoder_dropout`: (float) This is the dropout rate applied in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4248533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAG_GNN(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e09a3d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0f390",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `lr` (float): This is the learning rate for the optimizer. It determines the step size at each iteration while moving towards a minimum of a loss function.\n",
    "\n",
    "* `lr_decay` (int): This is the step size for the learning rate scheduler. The learning rate will be reduced every `lr_decay` number of epochs.\n",
    "\n",
    "* `gamma` (float): This is the factor by which the learning rate will be reduced at each step of the learning rate scheduler. A `gamma` of 1.0 means the learning rate will stay the same.\n",
    "\n",
    "* `tau_A` (float): This is a regularization parameter for the adjacency matrix. It controls the degree of sparsity in the learned graph structure. A higher value of `tau_A` would enforce more sparsity, i.e., it would encourage the learned graph to have fewer edges. Conversely, a lower value would result in a graph with more edges.\n",
    "\n",
    "* `lambda_A` (float): This is another regularization parameter for the adjacency matrix. It is used in the computation of the loss function. More specifically, it controls the contribution of the graph complexity to the overall loss. A higher value of `lambda_A` would mean that you are penalizing complex graphs more heavily. On the other hand, a lower value means you are more tolerant of complex graphs.\n",
    "\n",
    "* `c_A` (int): This is a parameter used in the adaptive learning rate mechanism. It scales the learning rate based on the change in the graph structure. A higher value of `c_A` would lead to a more drastic reduction in the learning rate when the graph structure changes, which could help with stability but might slow down learning. A lower value of `c_A`, on the other hand, means that the learning rate stays more constant even when the graph structure changes, which could speed up learning but might lead to instability.\n",
    "\n",
    "* `graph_threshold` (float): This is the threshold used to binarize the learned adjacency matrix. Edges with weights below this threshold are removed.\n",
    "\n",
    "* `h_tol` (str): This is the tolerance for the stopping criterion. The learning process stops when the computed `h(A)` (a measure of the complexity of the graph structure) is below this tolerance.\n",
    "\n",
    "* `k_max_iter` (int): This is the maximum number of iterations for the outer loop of the learning process.\n",
    "\n",
    "* `epochs` (int): This is the number of epochs for which the model will be trained. An epoch is a complete pass over the entire training dataset.\n",
    "\n",
    "* `optimizer` (str): This is the type of optimizer used for the learning process. It can be \n",
    "    * `Adam`, \n",
    "    * `LBFGS`, \n",
    "    * `SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb81e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, args, ground_truth_G, report_log=True)\n",
    "best_ELBO_graph, best_NLL_graph, best_MSE_graph = trainer.train(train_loader, use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779517f",
   "metadata": {},
   "source": [
    "## Evaluation (when ground truth is avaliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# fdr: false discovery rate, lower the better\n",
    "# tpr: true positive rate, higher the better\n",
    "# fpr: false positive rate, lower the better\n",
    "# shd: symetric hamming distance, lower the better\n",
    "# nnz: number of nonzeros, closer to the ground truth the better\n",
    "\"\"\"\n",
    "best_graph = {'ELBO':best_ELBO_graph, 'NLL':best_NLL_graph, 'MSE':best_MSE_graph}\n",
    "for key in best_graph:\n",
    "    fdr, tpr, fpr, shd, nnz = utils.count_accuracy(ground_truth_G, nx.DiGraph(best_graph[key]))\n",
    "    print('Best %s Graph Accuracy: fdr'%(key), fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "graph = trainer.origin_A.data.cpu().clone().numpy()\n",
    "# various graph threshold\n",
    "for thres in [0.1, 0.2, 0.3]:\n",
    "    graph[np.abs(graph) < thres] = 0\n",
    "    fdr, tpr, fpr, shd, nnz = utils.count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold %.1f, Accuracy: fdr'%(thres), fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
