{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae536a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit\n",
    "import gTDR.utils.GTS as utils\n",
    "from gTDR.trainers.GTS_trainer import Trainer\n",
    "from gTDR.models import GTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ab596",
   "metadata": {},
   "source": [
    "## Arguments & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21070d1",
   "metadata": {},
   "source": [
    "Specify the setup in config, including:\n",
    "* `base_dir`: (str) The path where to save the trained model and results.\n",
    "* `dataset_dir`: (str) The path of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/GTS_METR_LA_parameters.yaml\"\n",
    "args = yaml.load(open(config_filename), Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847dbc1",
   "metadata": {},
   "source": [
    "Start `wandb` for monitoring experiment (train loss, validation loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf19d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"GTS\", name=\"METR-LA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a8aa6",
   "metadata": {},
   "source": [
    "## Data (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36147452",
   "metadata": {},
   "source": [
    "In this demo, we use the `METR-LA` dataset.\n",
    "\n",
    "**First, unzip dataset located at [../data/METR-LA/](../data/METR-LA/):**\n",
    "\n",
    "`gunzip -c ../data/METR-LA/metr-la.h5.zip > ../data/METR-LA/metr-la.h5`\n",
    "\n",
    "**Then, run the script [./data_preparation/prepare_METR_LA.py](./data_preparation/prepare_METR_LA.py) to create data files `train.npz`, `val.npz`, and `test.npz` under [../data/METR-LA/](../data/METR-LA/).**\n",
    "\n",
    "`python ./data_preparation/prepare_METR_LA.py --traffic_df_filename ../data/METR-LA/metr-la.h5 --output_dir ../data/METR-LA/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cec866",
   "metadata": {},
   "source": [
    "## Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3be3c6",
   "metadata": {},
   "source": [
    "Next, load data files and create data.\n",
    "\n",
    "A dataloader class `DataLoader` has been built into the GTS codebase. This class contains a `get_iterator()` method.\n",
    "\n",
    "In addition, a `load_dataset()` function has been built into the GTS codebase (see [../gTDR/utils/GTS/utils.py](../gTDR/utils/GTS/utils.py)), which returns a dictionary `data` that contains many contents:\n",
    "* `x_train`, `x_val`, `x_test`: time series covariates\n",
    "* `y_train`, `y_val`, `y_test`: time series targets\n",
    "* `train_loader`, `val_loader`, `test_loader`: dataloaders\n",
    "* `scaler`: a method to perform feature normalization\n",
    "\n",
    "Due to the complexity of the implementation of `load_dataset()`, we do not show it here but directly call it. For a new dataset, one needs to modify [../gTDR/utils/GTS/utils.py](../gTDR/utils/GTS/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kwargs = args.get('data_para')\n",
    "data = utils.load_dataset(**data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f422e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e976e",
   "metadata": {},
   "source": [
    "You may specify these model parameters in config:\n",
    "\n",
    "* `use_curriculum_learning`: (bool) Whether to use curriculum learning strategy. If True, the model gradually increases the difficulty of training samples.\n",
    "\n",
    "* `cl_decay_steps`: (int) The number of steps for the curriculum learning decay. Curriculum learning is a type of training strategy that gradually increases the difficulty of training samples. The decay steps could determine how fast the \"lessons\" become more difficult.\n",
    "\n",
    "* `filter_type`: (str) The type of graph convolutional filter to use. This parameter would affect how the graph convolution operation is computed. The provided options are \n",
    "    * `dual_random_walk`, \n",
    "    * `random_walk`, \n",
    "    * `laplacian`. \n",
    "\n",
    "* `horizon`: (int) The prediction horizon of the model. This is the number of future time steps the model is trained to predict.\n",
    "\n",
    "* `input_dim`: (int) The dimensionality of the input data. This would be the number of features in the input data for each node at each time step.\n",
    "\n",
    "* `output_dim`: (int) The dimensionality of the output data. This would be the number of features the model is trained to predict for each node at each time step.\n",
    "\n",
    "* `l1_decay`: (float) The strength of L1 regularization applied during training. Regularization helps prevent overfitting by adding a penalty to the loss for large weights.\n",
    "\n",
    "* `max_diffusion_step`: (int) The maximum number of diffusion steps in the graph convolution operation. This parameter affects how far information travels along the graph in each layer of the GCN.\n",
    "\n",
    "* `num_nodes`: (int) The number of nodes in the graph. This is simply the number of different locations or sensors in your dataset.\n",
    "\n",
    "* `num_rnn_layers`: (int) The number of recurrent layers in the model. More layers can capture more complex patterns but also increase the risk of overfitting and the computational cost.\n",
    "\n",
    "* `rnn_units`: (int) The number of units in each recurrent layer. More units can capture more complex patterns but also increase the risk of overfitting and the computational cost.\n",
    "\n",
    "* `seq_len`: (int) The length of the input sequences. This is the number of past time steps the model uses to make its predictions.\n",
    "\n",
    "* `dim_fc`: (int) The size of the fully connected layer. This parameter affects the complexity and capacity of the model.\n",
    "\n",
    "* `temperature`: (float) This parameter is used in the context of the Gumbel reparameterization trick, which allows the model to handle discrete graph structures. This parameter controls the \"sharpness\" of the distribution from which the adjacency matrix of the graph is sampled.\n",
    "\n",
    "In the code below, the `temperature` is set to `0.5`. As the temperature approaches 0, the model's sampling of the adjacency matrix becomes more deterministic, tending to choose either 0 or 1 with higher probability. Conversely, a higher temperature value leads to a more uniform sampling, making the model's choices more exploratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTS(temperature=0.5, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa5750",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc0ecf",
   "metadata": {},
   "source": [
    "You may specify these training parameters in config:\n",
    "\n",
    "* `base_lr`: (float) The base learning rate for the optimizer.\n",
    "\n",
    "* `lr_decay_ratio`: (float) The ratio for learning rate decay.\n",
    "\n",
    "* `dropout`: (float) The dropout rate for regularization.\n",
    "\n",
    "* `epoch`: (int) The starting epoch number for training. If you are continuing training from a saved model, this would be the epoch at which training stopped. If you are training from scratch, it should be `0`.\n",
    "\n",
    "* `epochs`: (int) The total number of epochs (complete passes through the training dataset) to train the model.\n",
    "\n",
    "* `epsilon`: (float) A small constant for numerical stability in the optimizer.\n",
    "\n",
    "* `global_step`: (int) A counter for the total number of steps taken so far in training. This could be used for logging or for learning rate scheduling. If you are training from scratch, it should be `0`.\n",
    "\n",
    "* `max_grad_norm`: (float) The maximum allowed norm for the gradient clipping. This is used to prevent the problem of exploding gradients in deep neural networks.\n",
    "\n",
    "* `min_learning_rate`: (float) The minimum learning rate. This sets a lower bound on the learning rate, preventing it from going too low.\n",
    "\n",
    "* `optimizer`: (str) The optimizer to use for training. Common options are `adam`, `sgd`. If no optimizer is specified, the default is `adam`.\n",
    "\n",
    "* `patience`: (int) The number of epochs to wait for improvement before stopping training. This is used in early stopping.\n",
    "\n",
    "* `steps`: (list of int) The epochs at which to decrease the learning rate. For example, if set to [20, 30, 40], the learning rate would be decreased after 20, 30, and 40 epochs.\n",
    "\n",
    "* `test_every_n_epochs`: (int) The number of epochs after which to test the model.\n",
    "\n",
    "* `knn_k`: (int) The number of nearest neighbors to consider in KNN graph construction. This is used when constructing the adjacency matrix for the graph-based model.\n",
    "\n",
    "* `epoch_use_regularization`: (int) The epoch at which to start using regularization. This could be used to delay the use of regularization to allow the model to learn more freely in early epochs.\n",
    "\n",
    "* `num_sample`: (int) The number of samples to use in each training step. This could be used in methods that involve sampling from the model or the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d1419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, data, **args)\n",
    "trainer.train(use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a145ce",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b01bfa",
   "metadata": {},
   "source": [
    "Load the best check point and perform testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
